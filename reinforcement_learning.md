# Reinforcement Learning

## Terminology

- mean-squared Bellman error (MSBE)
- on/off-policy
  - An on-policy learning strategy uses data samples from the latest policy to update the policy. While an off-policy strategy uses samples generated by evaluating previous policies. Anything that uses a database of transitions (e.g. replay buffer) would typically be considered off-policy.
- temporal difference (TD)
- advantage (A)
  - The difference between the value given by taking a specific action at a given state, over the expected value over the distribution of actions at the same state.
  - $$A(s,t,\pi)=Q(s,t,\pi)-V(s,\pi)$$
- Markov decision process (MDP) - 

## Markov decision process (MDP)

An MDP is defined by the following elements

- $S$ - the state space
- $A$ - the action space
- $x_t\in S$ - the state at time $t$
- $a_t\in A$ - the action taken at time $t$
- $f_t\in\mathbb{R}$ - the feedback at time $t$
- $\pi:S\times A\rightarrow\mathbb{R}$ - the policy function giving the probability of taking an action at a given state
- $P:S\times A\times S\rightarrow\mathbb{R}$ - the transition function giving the probability of transition to a given state after taking an action at a previous state
- $J(\pi)=\mathbb{E}\left(\sum_{t=0}^{\infty}\beta^t f_t|\pi\right)$ - total discounted return, where $\beta$ is the discount factor.

## Bellman Equation

https://en.wikipedia.org/wiki/Bellman_equation

### Value function

We define the maximum possible discounted value achievable starting from some initial state as:

$$V(x_0)=\max_{\{a_t\}}\sum_{t=0}^\infty\beta^t F(x_t,a_t)$$

s.t.

$$a_t\in\Gamma(x_t)$$
$$x_{t+1}=T(x_t,a_t)$$

Where

- $x_t$ is the state at time $t$
- $a_t\in\Gamma(x_t)$ is the action taken at time $t$
- $\Gamma(x)$ is the possible actions at state $x$.
- $F(x,a)$ is the feedback/reward/payoff function for taking action $a$ in state $x$.
- $T(x,a)$ is the transition function giving the next state after taking action $a$ in state $x$.
- $\beta$ is the discount factor, used to reduce the value of feedback in the future.

### Recursive value function

We can rewrite the value function in recursive form:

$$V(x_0)=\max_{a_0}(F(x_0,a_0)+\beta V(x_1))$$

Or more compactly:

$$V(x)=\max_{a\in\Gamma(x)}(F(x,a)+\beta V(T(x,a)))$$

### Stochastic value function

If we assume that the state transitions (T) and feedback (F) are stochastic we define the value function instead using the expectation as follows:

$$V(x)=\max_{a\in\Gamma(x)}(\mathbb{E}(F(x,a)+\beta(V(T(x,a)))))$$

## Q-Learning

In this case we use a modified Bellman equation to take in both a state, an action, the current policy parameters and return the expected return.

$$\hat{Q}(x,a,\pi)=\mathbb{E}(F(x,a)+\beta\hat{Q}(T(x,\pi(x))))\$$

Where

- $\pi$ is the policy.

In the case of reinforcement learning, we directly learn an approximation to the Q function,

$$Q(x,a,\theta)=\hat{Q}(x,a,\pi(\theta))$$

Where

- $\theta$ are the parameters that encode both the composite policy and value function.

In this case we take a step and then our objective is the Bellman Error as follows:

$$|Q(x_t,a_t,\theta)-(F_t+\beta Q(T_t,\pi(T_t),\theta))|$$

Where

- $F_t=F(x_t,a_t)$ is the feedback we have observed.
- $T_t=T(x_t,a_t)$ is the transition we have observed.

Additionally, while there are further steps to implement an exploit/explore execution algorithm, the above equation gives supervised learning approach for fitting $\tilde{Q}(\theta,\cdot)$.

References:
- https://en.wikipedia.org/wiki/Q-learning
- https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf
- https://arxiv.org/abs/2410.21795

## Learning stability

One of the most significant challenges of reinforcement learning is the ability of the fitting process to arrive at a stable solution. Because of the recursive formulation, we end up with a problem that is not purely a $f(x_i)=y_i$ supervised learning formulation which is generally convergent with enough regularization and a small enough learning rate. Instead, we end up with a recursive formulation where our labels are the step values, $f(x_i,a_i)=y_i+\beta f(x'_i,\argmax_a f(x'_i,a))=y_i+\beta\max_a f(x'_i,a)$. Ideally this would converge to the non-recursive definition, but can often diverge due to the dynamics of the optimization process, where the portion of the residual due to the $y_i$ term is not minimized. Instead, the portion due to the recursive term dominates, and since it has no grounding in supervised data, it can diverge to unuseful or even unbounded solutions.

## Exploitation vs exploration

### $\epsilon$-greedy (epsilon greedy)

In this strategy we simply take a random action at a fixed rate, which is defined as $\epsilon\in[0,1]$. So effectively we have:

$$a_t(x_t)=\left\{\begin{matrix}a \sim \mathbb{U}(\Gamma(x_t)), & P(\cdot)=\epsilon \\ \displaystyle\max_{a\in\Gamma(x_t)}V(x,a), & P(\cdot)=1-\epsilon\ \end{matrix}\right.$$

### Boltzmann exploration

$$\pi(s,a)=exp(Q(s,a)/T)\int_{b\in\Gamma(x_t)} exp(Q(s,b)/T)db$$

$$a_t(x_t)\sim\pi(s,a)$$

## Online vs Offline Operation

We use equation 9 as the objective when learning the parameters for Q. Generically we can structure our dataset as follows:

$$\{x_t^i,a_t^i,F_t^i,T_t^i\},i\in[1,N]\$$

With this definition we can utilize a normal training loop using pre-generated data, or by running simulation within the training loop at additional cost, but increased robustness.

```
class PrerecordedDataGenerator:
  det __init__(self,...):
    #Load from files
    self.data = [(...),
                 (...)]

  def Get(self,model):
    return random.choose(dataset)

class OnlineDataGenerator:
  def __init__(self,...):
    ...

  def Get(self,model):
    #

def train(data_generator):
  while True:
    sample=data_generator.Get(model)
    objective=model.Q(sample.x_t,sample.a_t)-(sample.F_t+model.beta*model.Q(sample.T_t,model.policy(sample.T_t)))

    parameter_derivatives=differentiate(objective,model.parameters())

    updater.update(model.parameters(),parameter_derivatives)
```

## Implicit vs explicit models (sampling optimal policies)

Implicit:

From a theoretical perspective, and as listed above, Q functions typically both the state and action as arguments, and then return the value. However, this can be expensive if you want to find the optimal policy as you need to solve an optimization problem:

$$Q(x,a)\in\real$$

$$\pi_\text{opt}(x)=\text{argmax}_a Q(x,a)$$

This can require a non-trivial optimizer such as gradient descent that may require many function/gradient evaluations and get stuck in local minima.

Explicit:

Alternatively for discrete action spaces, you can simply return a vector containing the value for each possible action:

$$Q(x)=\begin{pmatrix}Q(x,a_1) \\ \vdots \\ Q(x,a_n)\end{pmatrix}$$

## Continuous/discontinuous/composite action spaces

One of the challenges with Q/Value learning is how to represent the action space. For simple systems, the action may simply be a choice from $N$ options at each step. However, for most real world systems there are a number of issues:

- Actions are typically a composite of a number of sub-actions (e.g. $a=(a0,a1)$). This can be resolved by representing the action as indexing into the Cartesian product of the sub-actions.
- Actions can have real-valued sub-actions. This can be resolved by either using an implicit value representation or by quantizing the action space.

When the above 2 issues are combined we can have very large action spaces. For example if we 4 sub-actions each with 5 options, then the action space will be $5^4=625$. This can negatively impact the learning process as it takes a very long time to explore the action space, even when most of the actions are highly correlated (as in the real-valued case).

Alternatively you can use an implicit Q model (the critic), and to overcome the issue of finding the optimal action, use an explicit policy (the actor).

So we end up with the following optimization problem:

$$\min_{Q,\pi} \sum_{(x_t,a_t,F_t,x_{t+1})} (Q(x_t,a_t)-(F_t+\beta Q(x_{t+1},\pi(x_{t+1})))) - Q(x_t,\pi(x_t))$$

Where the first term is the Bellman Error and the second term optimizes the policy. Once we have a solution to this minimization, we can simply use the policy $\pi(x)$ to one-shot generate optimal actions.

References:
- https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic

## Deep Q-Learning/Network (DQN)

Uses a neural network to model the Q function as a map from a continuous valued state to the discrete action values: $Q(x)=a: \real^{N_x} -> \real^{N_a}$

## Deep Deterministic Policy Gradient (DDPG)

https://arxiv.org/abs/1509.02971

This extends DQN to continuous valued actions. It also stabilizes actor-critic updates by maintaining slow moving target models. The parameters of these target models are updated with a linear update as

$$\bar{\theta}^{n+1}=\tau\theta^{n+1}+(1-\tau)\bar{\theta}^n$$

Where $\tau$ is the target learning rate, typically closer to $0$. These are then used to sample actions/values when generating targets for the primary models. The goal of this method is to stabilize the learning process.

## Distributed Distributional Deep Deterministic Policy Gradient (D4PG)

- Separate agent and training processes
- N-Step returns - Q/V functions modified to have N steps before become recursive
- Prioritized training samples via importance sampling

https://arxiv.org/abs/1804.08617

## Prioritized Replay Experience

https://arxiv.org/abs/1511.05952

## Soft Actor-Critic (SAC)

https://arxiv.org/abs/1801.01290

## Proximal Policy Optimization (PPO)

https://arxiv.org/abs/1707.06347

## Group-Relative Policy Optimization (GRPO)

https://arxiv.org/pdf/2402.03300

## Asychronous RL

https://arxiv.org/abs/1602.01783

## Combining policy gradient and Q-learning

- https://arxiv.org/abs/1611.01626

## Avoiding actor updates from exploiting poor critic approximations

### Advantage weighted regression (AWR)

https://arxiv.org/abs/1910.00177

### Conservative Q-Learning (CQL)

https://arxiv.org/abs/2006.04779

TODO: Document

- $\mathcal{D}$ is the dataset.
- $\pi_\beta(a|s)$ is the stochastic policy.
- $d^{\pi_\beta}(s)$ is the discounted marginal state-distribution of $\pi_\beta(a|s)$.
- $P^\pi Q(s,a)=\mathbb{E}_{s'\sim T(s'|s,a),a'\sim\pi(a'|s')} Q(s',a')$


## Reparameterization trick

TODO

## State distribution

- Trajectory distribution: $$p^\pi(\tau)=p(s_0)\prod_{t=0}^{T}p(s_{t+1}|s_t,a_t)\pi(a_t|s_t)$$
  - A trajectory distribution gives the probability of taking a specific trajectory given a policy distribution function and a state transition distribution function.
- State marginal: $p^\pi(s_t=s)=p(s_t=s)=\displaystyle\sum_{s_0}p(s_0)\sum_{a_0}\pi(a_0|s_0)\sum_{s_1}p(s_1|s_0,a_0)\sum_{a_1}\pi(a_1|s_1)...\sum_{s_{t-1}}p(s_{t-1}|s_{t-2},a_{t-2})\sum_{a_{t-1}}\pi(s_t|s_{t-1},a_{t-1})\sum_{a_t}\pi(a_t|s_t)\sum_{s_{t+1}}p(s_t+1|s_t,a_t)...$
  - This gives the probability of having a sampled trajectory end up at state $s$ at time $t$.
- Discounted stationary distribution: $d^\pi(s)=(1-\beta)\displaystyle\sum_{t=0}^{\infty}\beta^{t}p^\pi(s_t=s)=(1-\beta)p(s_0=s)+\beta\sum_{s\in\mathcal{S},a\in\mathcal{A}}\mathcal{T}(s'|s,a)\pi(a|s),d^\pi(s)$

References:
- https://maximiliandu.com/course_notes/DRL/Deep%20RL/Notes/The%20Key%20Distributions%2000672c28d27043e2bafdbb30ff38423e.html

## Bellman operators

- Bellman optimality operator: $B^*Q(s,a)=f(s,a)+\beta\mathbb{E}s'~P(s'|s,a)\left(\max_a' Q(s',a')\right)$
- Bellman operator: $B^\pi Q=f+\beta P^\pi Q$ where $P^\pi$ is the transition matrix:
  - $P^\pi Q(s,a)=\mathbb{E}_{s'\sim T(s'|s,a),a'\sim\pi(a'|s')}Q(s',a')$
- Single sample Bellman operator: $\hat{B}^\pi Q=f(s,a)+\beta Q(s',a\sim\pi(s'))$ or, in the case of an explicit policy function, $\hat{B}^\pi Q=f(s,a)+\beta Q(s',\pi(s'))$
  - The hat means approximate statistics with a single sample.
  - $\pi(s')$ here is used in the case of an explicit policy function that produces a currently optimal action rather than an action probability distribution. This is equivalent to the statement $\pi: \R^{N_s}->\R^{N_a}$, where $N_s$ is the dimension of the state space and $N_a$ is the dimension of the action space.

## Debugging notes

- If you see spikes in the Q objective try these steps:
  1. Check that the state/action values are within specific ranges. These are dictated by the environment dynamics so a problem here indicates a problem with the dynamics. If they are large it may be due to the simulation environment's own integration going unstable. By default Mujoco uses explicit integration, this combined with penalty forces for constraints has a very narrow stability regime which an RL agent with large force limits can generally exceed very quickly. Set Mujoco to use implicit time integration.
  2. Check the feedback values. These are directly included in the objective so they can easily produce a large objective if they themselves are large. However, they are also specified by a heuristic, so check those if the feedback looks large.
  3. Run with a smaller learning rate. If the spikes disappear, then the learning rate was too large and the solver was going unstable.
  4. Increase regularization. This can help the solver avoid producing a model with chaotic subspaces that may produce large values when newly explored.
- If your policy's value function is not showing an improvement over time, try these steps:
  1. View the behavior of the system. Does it look like it's getting into a stable regime associated with a local minima? Try different exploration policies.
  2. Check the Q error for each step and compare against the Q error shown by the solver on the batch from the replay buffer. If the replay buffer error is much smaller then relevant data points are not being sampled within the solver.
  3. In theory if a bad policy is continually exploited, it should generate enough datapoints

https://en.wikipedia.org/wiki/Multi-objective_optimization
